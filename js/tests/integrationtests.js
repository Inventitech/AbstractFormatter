module( "Integration tests" );

test('MSR Abstract from PDF', function() {
    var input = 'Code review is the manual assessment of source code by humans,\nmainly intended to identify defects and quality problems. Modern\nCode Review (MCR), a lightweight variant of the code inspections\ninvestigated since the 1970s, prevails today both in industry and\nopen-source software (OSS) systems. The objective of this paper is\nto increase our understanding of the practical benefits that the MCR\nprocess produces on reviewed source code. To that end, we empiri-\ncally explore the problems fixed through MCR in OSS systems. We\nmanually classified over 1,400 changes taking place in reviewed\ncode from two OSS projects into a validated categorization scheme.\nSurprisingly, results show that the types of changes due to the MCR\nprocess in OSS are strikingly similar to those in the industry and\nacademic systems from literature, featuring the similar 75:25 ratio\nof maintainability-related to functional problems. We also reveal\nthat 7–35% of review comments are discarded and that 10–22%\nof the changes are not triggered by an explicit review comment.\nPatterns emerged in the review data; we investigated them revealing\nthe technical factors that influence the number of changes due to the\nMCR process. We found that bug-fixing tasks lead to fewer changes\nand tasks with more altered files and a higher code churn have more\nchanges. Contrary to intuition, the person of the reviewer had no\nimpact on the number of changes.';
    var expected = 'Code review is the manual assessment of source code by humans, mainly intended to identify defects and quality problems. Modern Code Review (MCR), a lightweight variant of the code inspections investigated since the 1970s, prevails today both in industry and open-source software (OSS) systems. The objective of this paper is to increase our understanding of the practical benefits that the MCR process produces on reviewed source code. To that end, we empirically explore the problems fixed through MCR in OSS systems. We manually classified over 1,400 changes taking place in reviewed code from two OSS projects into a validated categorization scheme. Surprisingly, results show that the types of changes due to the MCR process in OSS are strikingly similar to those in the industry and academic systems from literature, featuring the similar 75:25 ratio of maintainability-related to functional problems. We also reveal that 7–35% of review comments are discarded and that 10–22% of the changes are not triggered by an explicit review comment. Patterns emerged in the review data; we investigated them revealing the technical factors that influence the number of changes due to the MCR process. We found that bug-fixing tasks lead to fewer changes and tasks with more altered files and a higher code churn have more changes. Contrary to intuition, the person of the reviewer had no impact on the number of changes.';
    equal(formatText(input), expected, 'Test on MSR Abstract (PDF)');
});

test('Abstract Master\'s Thesis TK', function() {
    var input = '% Abstract\n\\chapter*{Abstract}\nResearch in software engineering has shown that the reuse of software components reduces bugs, improves code quality and decreases development time.\nIn the past many code search systems have been proposed to help developers find code that is suitable for reuse in large code bases. Today large companies see several commits to their software repositories every minute and developers expect information they rely on to be up to date. This creates the need for a novel approach of analyzing the code base and keeping the code search engine updated. The traditional approach to read the entire code base becomes difficult as updating takes longer than the time between two subsequent commits to the repository.\n%In the past many code search systems have been proposed to help developers find code that is suitable for reuse in large code bases. Today large companies see several commits to their software repositories every minute. This creates the need for a novel approach of analyzing the code base and keeping the code search engine updated. The traditional approach to read the entire code base becomes difficult as updating takes longer than the time between two subsequent commits to the repository.\nWhile other code search systems focus on text-based search or use test-cases to find suitable code for reuse, we build on the approach of using a method\'s context to find useful methods for reuse. We complement this approach with signature matching and techniques known from software engineering: architecture analysis and code review states of code. These are used as metrics to rate a method\'s suitability in a given context.\nThis thesis contributes a novel code search system designed for use with companies\' internal code bases. The proposed system uses an incremental approach to update its index, updating only parts of data affected by the changes in each commit. This keeps the analysis time low and allows the system to work with updated data in the matter of seconds.\nTo evaluate the proposed code search system a novel evaluation method is introduced that is built on the concept of incremental analysis. It enables the usage of a system\'s entire development history, accurately replaying the actual development process. The  system\'s state and the changes made at any given commit can be used to accurately evaluate the code search system using the data that would have been available at that time.\nThe evaluation shows that our code search system consistently delivers results containing the developers choice on two different systems in more than 50\% of the cases when retrieving 5 results. We evaluate all combinations of our proposed metrics and show which lead to the best results.';
    var expected = 'Abstract Research in software engineering has shown that the reuse of software components reduces bugs, improves code quality and decreases development time. In the past many code search systems have been proposed to help developers find code that is suitable for reuse in large code bases. Today large companies see several commits to their software repositories every minute and developers expect information they rely on to be up to date. This creates the need for a novel approach of analyzing the code base and keeping the code search engine updated. The traditional approach to read the entire code base becomes difficult as updating takes longer than the time between two subsequent commits to the repository. While other code search systems focus on text-based search or use test-cases to find suitable code for reuse, we build on the approach of using a method\'s context to find useful methods for reuse. We complement this approach with signature matching and techniques known from software engineering: architecture analysis and code review states of code. These are used as metrics to rate a method\'s suitability in a given context. This thesis contributes a novel code search system designed for use with companies\' internal code bases. The proposed system uses an incremental approach to update its index, updating only parts of data affected by the changes in each commit. This keeps the analysis time low and allows the system to work with updated data in the matter of seconds. To evaluate the proposed code search system a novel evaluation method is introduced that is built on the concept of incremental analysis. It enables the usage of a system\'s entire development history, accurately replaying the actual development process. The system\'s state and the changes made at any given commit can be used to accurately evaluate the code search system using the data that would have been available at that time. The evaluation shows that our code search system consistently delivers results containing the developers choice on two different systems in more than 50% of the cases when retrieving 5 results. We evaluate all combinations of our proposed metrics and show which lead to the best results.';
    equal(formatText(input), expected, 'Test on TK\'s Master\'s Thesis Abstract (Latex)');
});
test('Abstract Thesis Tiago Espinha', function() {
    flattenParagraphs = false;
    var input = '\n\nAbstract\nAt an implementation level, web services serve the basic purpose of message exchange between potentially heterogeneous software systems. Through abstracting language- and platform-specific implementations into text-based, human-readable XML and JSON-based formats, different software systems are able to execute procedures and retrieve data from remote systems, in many cases provided by a third-party.\n\nIn this thesis, we analyze web services from two perspectives: web services used to provide an interface with which third-party clients can integrate, and web services used as the components used to build a software system. This distinction is made in the sections below together with the different challenges addressed in this thesis.\n\nWeb Services for Integration \nWhen studying web services as a means for integration, the major challenge we address in this thesis relates to how software systems naturally evolve to keep up with ever-changing laws, services and technologies. When using a static software library such as a Java ARchive (JAR), it is up to the client developer to decide when and if to integrate such new "evolved" versions of the library. Therefore, any effort of integrating a new version could be delayed at the developer\'s own discretion.\n\nWhen integrating a client with web services, this is no longer the case. It is then the web service \emph{provider} who decides when the web service will bear new features and/or (potentially breaking) changes in behavior. It is also the web service provider who decides whether the older versions will remain accessible and for how long. In such a scenario, client developers must deal with an added pressure of conformity. If their client is not compatible with the new version it may simply stop to work when support for the older version is removed. \nThis power-shift in who controls the evolution pace of service integration led us to study two facets of this client/provider relationship:\n\nWeb Service Providers. We investigated whether this power-shift actually happens in practice by studying real world examples of breaking changes pushed by high-profile web service providers (Facebook, Twitter, ...) and the impact they have on client source code. We found that, indeed, in many of these instances the changes are breaking and invasive. Moreover, some client developers are also unhappy with both the structuring and frequency with which web service providers structure their (breaking) changes. Another interesting finding is how web service providers lack standard best-practices which all the web service providers agree on. Instead, each web service provider independently decides on which policies they will follow regarding evolution, backwards compatibility and versioning (or lack of versioning).\n\nWeb Service Client Developers. Seeing as client developers are sometimes inconvenienced by breaking changes, we also investigated whether they have then developed resilience against potentially unstable and frequently changing web services. To achieve this we make use of mutation analysis to simulate evolving and failing web services and observe the behavior of a set of Android applications which integrate with these web services. While the results are mixed, a considerable number of applications crashed upon facing these changes which hints that not all client developers are aware of the added responsibilities when integrating a third-party web service.\n\nWeb Services for Service-Orientation\nWeb services are also used as building blocks for software systems in the so-called Software Oriented Architectures (SOA). When that is the case, it is often difficult (and at times impossible altogether) to understand what are the repercussions of changing the functionality of a specific web service. This is due to the loosely coupled nature of web services which can, in some cases, resolve dependencies at runtime. In order to address this, we created and evaluated an implementation of a runtime topology (Serviz) which provides us with this information. \nThe runtime topology provides the system maintainer with an overview of which web services communicated with which other web services. By having this overview of causality in web service requests and therefore which web service methods depend on which other methods, system maintainers are then better able to understand which users and which other web service methods are affected when performing maintenance on a specific web service method.Our runtime topology provides such an overview while also allowing for different types of filtering (which can be combined):\n\n- Time-based filtering, which allows for restricting the interactions to a specific period of time.\n\n- Service-based filtering, which in large service-based systems helps in pinpointing all interactions and thus other web services which are invoked together with a particular web service.\n\n- Version-based filtering, which allows the filtering to be done with a higher degree of granularity and for choice to be done on a particular version of a web service.\n\n- User-based filtering, which shows the web service interactions only for a single user. \n\nBesides the runtime topology, a usage graph is also computed per web service which allows system maintainers to visually identify high and low usage peaks. Such graph is potentially useful for identifying the potentially best periods for performing software maintenance.\n\nConclusion \nUltimately, whether for an integration or service-orientation scenario, our results suggest that the lack of backwards compatibility in an environment where services actively depend on each other is one of the major causes of pain for web service client developers. An aspect which remains yet to be explored is whether providing such backwards compatibility is feasible and at what cost and effort does it come for web service providers.';
    var expected = "<p>At an implementation level, web services serve the basic purpose of message exchange between potentially heterogeneous software systems. Through abstracting language- and platform-specific implementations into text-based, human-readable XML and JSON-based formats, different software systems are able to execute procedures and retrieve data from remote systems, in many cases provided by a third-party.</p><p>In this thesis, we analyze web services from two perspectives: web services used to provide an interface with which third-party clients can integrate, and web services used as the components used to build a software system. This distinction is made in the sections below together with the different challenges addressed in this thesis.</p><p>Web Services for Integration When studying web services as a means for integration, the major challenge we address in this thesis relates to how software systems naturally evolve to keep up with ever-changing laws, services and technologies. When using a static software library such as a Java ARchive (JAR), it is up to the client developer to decide when and if to integrate such new \"evolved\" versions of the library. Therefore, any effort of integrating a new version could be delayed at the developer's own discretion.</p><p>When integrating a client with web services, this is no longer the case. It is then the web service emph{provider} who decides when the web service will bear new features and/or (potentially breaking) changes in behavior. It is also the web service provider who decides whether the older versions will remain accessible and for how long. In such a scenario, client developers must deal with an added pressure of conformity. If their client is not compatible with the new version it may simply stop to work when support for the older version is removed. This power-shift in who controls the evolution pace of service integration led us to study two facets of this client/provider relationship:</p><p>Web Service Providers. We investigated whether this power-shift actually happens in practice by studying real world examples of breaking changes pushed by high-profile web service providers (Facebook, Twitter, ...) and the impact they have on client source code. We found that, indeed, in many of these instances the changes are breaking and invasive. Moreover, some client developers are also unhappy with both the structuring and frequency with which web service providers structure their (breaking) changes. Another interesting finding is how web service providers lack standard best-practices which all the web service providers agree on. Instead, each web service provider independently decides on which policies they will follow regarding evolution, backwards compatibility and versioning (or lack of versioning).</p><p>Web Service Client Developers. Seeing as client developers are sometimes inconvenienced by breaking changes, we also investigated whether they have then developed resilience against potentially unstable and frequently changing web services. To achieve this we make use of mutation analysis to simulate evolving and failing web services and observe the behavior of a set of Android applications which integrate with these web services. While the results are mixed, a considerable number of applications crashed upon facing these changes which hints that not all client developers are aware of the added responsibilities when integrating a third-party web service.</p><p>Web Services for Service-Orientation Web services are also used as building blocks for software systems in the so-called Software Oriented Architectures (SOA). When that is the case, it is often difficult (and at times impossible altogether) to understand what are the repercussions of changing the functionality of a specific web service. This is due to the loosely coupled nature of web services which can, in some cases, resolve dependencies at runtime. In order to address this, we created and evaluated an implementation of a runtime topology (Serviz) which provides us with this information. The runtime topology provides the system maintainer with an overview of which web services communicated with which other web services. By having this overview of causality in web service requests and therefore which web service methods depend on which other methods, system maintainers are then better able to understand which users and which other web service methods are affected when performing maintenance on a specific web service method.Our runtime topology provides such an overview while also allowing for different types of filtering (which can be combined):</p><p>- Time-based filtering, which allows for restricting the interactions to a specific period of time.</p><p>- Service-based filtering, which in large service-based systems helps in pinpointing all interactions and thus other web services which are invoked together with a particular web service.</p><p>- Version-based filtering, which allows the filtering to be done with a higher degree of granularity and for choice to be done on a particular version of a web service.</p><p>- User-based filtering, which shows the web service interactions only for a single user. Besides the runtime topology, a usage graph is also computed per web service which allows system maintainers to visually identify high and low usage peaks. Such graph is potentially useful for identifying the potentially best periods for performing software maintenance.</p><p>Conclusion Ultimately, whether for an integration or service-orientation scenario, our results suggest that the lack of backwards compatibility in an environment where services actively depend on each other is one of the major causes of pain for web service client developers. An aspect which remains yet to be explored is whether providing such backwards compatibility is feasible and at what cost and effort does it come for web service providers.</p>"

;
    equal(formatText(input), expected, 'Test on Tiago Espinhas Thesis Abstract (Latex)');
    flattenParagraphs = true;
});
test('Analyze This Paper Quote', function() {
    flattenParagraphs = false;
    var input = '\n1. In the first survey, we asked a random sample of 1,500 Mi-\ncrosoft engineers a question similar to Greg Wilson’s. We\nasked “Please list up to five questions you would like [a team of data scientists who specialize in studying how software is developed] to answer.” After received 728 response items from 203 software engineers, we filtered and grouped them\ninto 679 questions in 12 categories. We then distilled these\ninto 145 descriptive questions (Section 4.1).\n\n2. We deployed a second survey to a new sample of 2,500 Mi-\ncrosoft engineers to help us prioritize the 145 descriptive ques-\ntions by indicating the most important ones to work on. We\nreceived 16,765 ratings from 607 Microsoft engineers. Theseratings additionally enabled us to identify differences of opin-ion between various demographic groups, for example, ques-\ntions that were more important to testers than to developers (Sections 4.2 and 4.3).\n\nThe scale of our study is larger: the findings in this paper are based\non input from 800+ software professionals in many different roles\nacross three engineering disciplines (development, testing, and pro-\ngram management).\n\nWe sent out two pilot surveys to 25 and 75 Microsoft en-\ngineers and used the responses to improve our questions.\n\nWe sent the initial survey to 1,500 Microsoft software engineers\nrandomly chosen by discipline in September 2012. Since monetary incentives have been found to increase the participation in surveys [27], we offered survey recipients the opportunity to enter a raffle for a $250 Visa Check Card. We received 728 items in 203 re-\nsponses, for a response rate of 13.5%.';
    var expected = "<p>1. In the first survey, we asked a random sample of 1,500 Microsoft engineers a question similar to Greg Wilson’s. We asked “Please list up to five questions you would like [a team of data scientists who specialize in studying how software is developed] to answer.” After received 728 response items from 203 software engineers, we filtered and grouped them into 679 questions in 12 categories. We then distilled these into 145 descriptive questions (Section 4.1).</p><p>2. We deployed a second survey to a new sample of 2,500 Microsoft engineers to help us prioritize the 145 descriptive questions by indicating the most important ones to work on. We received 16,765 ratings from 607 Microsoft engineers. Theseratings additionally enabled us to identify differences of opin-ion between various demographic groups, for example, questions that were more important to testers than to developers (Sections 4.2 and 4.3).</p><p>The scale of our study is larger: the findings in this paper are based on input from 800+ software professionals in many different roles across three engineering disciplines (development, testing, and program management).</p><p>We sent out two pilot surveys to 25 and 75 Microsoft engineers and used the responses to improve our questions.</p><p>We sent the initial survey to 1,500 Microsoft software engineers randomly chosen by discipline in September 2012. Since monetary incentives have been found to increase the participation in surveys [27], we offered survey recipients the opportunity to enter a raffle for a $250 Visa Check Card. We received 728 items in 203 responses, for a response rate of 13.5%.</p>";
    equal(formatText(input), expected, 'Test on Analyze This Paper Quote (PDF)');
    flattenParagraphs = true;
});
